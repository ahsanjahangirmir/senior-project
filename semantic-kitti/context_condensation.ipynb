{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013ab9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 00\n",
      "00: 4541 frames, keeping every 6th frame.\n",
      "frame_summaries.json 4541 to 757 frames.\n",
      "sequence_summary.json time_series -> 4541 to 757 entries\n",
      "File size before: 14823.30 KB, after: 2464.62 KB\n",
      "Token count before: 3308003, after: 550069\n",
      "FPS before: 10.0, after: 1.6666666666666667\n",
      "FPS reduction: 1.67 FPS\n",
      "Token reduction: 83.37%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 01\n",
      "01: 1101 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 1101 to 1101 frames.\n",
      "sequence_summary.json time_series -> 1101 to 1101 entries\n",
      "File size before: 1654.68 KB, after: 1654.68 KB\n",
      "Token count before: 402920, after: 402920\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 02\n",
      "02: 4661 frames, keeping every 6th frame.\n",
      "frame_summaries.json 4661 to 777 frames.\n",
      "sequence_summary.json time_series -> 4661 to 777 entries\n",
      "File size before: 10118.25 KB, after: 1688.68 KB\n",
      "Token count before: 2447889, after: 408691\n",
      "FPS before: 10.0, after: 1.6666666666666667\n",
      "FPS reduction: 1.67 FPS\n",
      "Token reduction: 83.30%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 03\n",
      "03: 801 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 801 to 801 frames.\n",
      "sequence_summary.json time_series -> 801 to 801 entries\n",
      "File size before: 1561.52 KB, after: 1561.52 KB\n",
      "Token count before: 387971, after: 387971\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 04\n",
      "04: 271 frames, keeping all (no condensation).\n",
      "frame_summaries.json 271 to 271 frames.\n",
      "sequence_summary.json time_series -> 271 to 271 entries\n",
      "File size before: 549.42 KB, after: 549.42 KB\n",
      "Token count before: 132722, after: 132722\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 05\n",
      "05: 2761 frames, keeping every 4th frame.\n",
      "frame_summaries.json 2761 to 691 frames.\n",
      "sequence_summary.json time_series -> 2761 to 691 entries\n",
      "File size before: 6708.92 KB, after: 1680.43 KB\n",
      "Token count before: 1598583, after: 400397\n",
      "FPS before: 10.0, after: 2.5\n",
      "FPS reduction: 2.50 FPS\n",
      "Token reduction: 74.95%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 06\n",
      "06: 1101 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 1101 to 1101 frames.\n",
      "sequence_summary.json time_series -> 1101 to 1101 entries\n",
      "File size before: 3466.50 KB, after: 3466.50 KB\n",
      "Token count before: 806003, after: 806003\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 07\n",
      "07: 1101 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 1101 to 1101 frames.\n",
      "sequence_summary.json time_series -> 1101 to 1101 entries\n",
      "File size before: 3829.31 KB, after: 3829.31 KB\n",
      "Token count before: 851157, after: 851157\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 08\n",
      "08: 4071 frames, keeping every 6th frame.\n",
      "frame_summaries.json 4071 to 679 frames.\n",
      "sequence_summary.json time_series -> 4071 to 679 entries\n",
      "File size before: 12501.21 KB, after: 2087.41 KB\n",
      "Token count before: 2856167, after: 476924\n",
      "FPS before: 10.0, after: 1.6666666666666667\n",
      "FPS reduction: 1.67 FPS\n",
      "Token reduction: 83.30%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 09\n",
      "09: 1591 frames, keeping every 2nd frame.\n",
      "frame_summaries.json 1591 to 796 frames.\n",
      "sequence_summary.json time_series -> 1591 to 796 entries\n",
      "File size before: 4067.31 KB, after: 2034.33 KB\n",
      "Token count before: 962304, after: 481341\n",
      "FPS before: 10.0, after: 5.0\n",
      "FPS reduction: 5.00 FPS\n",
      "Token reduction: 49.98%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 10\n",
      "10: 1201 frames, keeping every 2nd frame.\n",
      "frame_summaries.json 1201 to 601 frames.\n",
      "sequence_summary.json time_series -> 1201 to 601 entries\n",
      "File size before: 2658.93 KB, after: 1330.50 KB\n",
      "Token count before: 649071, after: 324801\n",
      "FPS before: 10.0, after: 5.0\n",
      "FPS reduction: 5.00 FPS\n",
      "Token reduction: 49.96%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 11\n",
      "11: 921 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 921 to 921 frames.\n",
      "sequence_summary.json time_series -> 921 to 921 entries\n",
      "File size before: 1382.06 KB, after: 1382.06 KB\n",
      "Token count before: 348753, after: 348753\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 12\n",
      "12: 1061 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 1061 to 1061 frames.\n",
      "sequence_summary.json time_series -> 1061 to 1061 entries\n",
      "File size before: 1388.14 KB, after: 1388.14 KB\n",
      "Token count before: 346420, after: 346420\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 13\n",
      "13: 3281 frames, keeping every 5th frame.\n",
      "frame_summaries.json 3281 to 657 frames.\n",
      "sequence_summary.json time_series -> 3281 to 657 entries\n",
      "File size before: 6068.10 KB, after: 1214.71 KB\n",
      "Token count before: 1557708, after: 311858\n",
      "FPS before: 10.0, after: 2.0\n",
      "FPS reduction: 2.00 FPS\n",
      "Token reduction: 79.98%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 14\n",
      "14: 631 frames, keeping all (no condensation).\n",
      "frame_summaries.json 631 to 631 frames.\n",
      "sequence_summary.json time_series -> 631 to 631 entries\n",
      "File size before: 736.87 KB, after: 736.87 KB\n",
      "Token count before: 181502, after: 181502\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 15\n",
      "15: 1901 frames, keeping every 3th frame.\n",
      "frame_summaries.json 1901 to 634 frames.\n",
      "sequence_summary.json time_series -> 1901 to 634 entries\n",
      "File size before: 2964.57 KB, after: 993.52 KB\n",
      "Token count before: 747271, after: 250512\n",
      "FPS before: 10.0, after: 3.3333333333333335\n",
      "FPS reduction: 3.33 FPS\n",
      "Token reduction: 66.48%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 16\n",
      "16: 1731 frames, keeping every 3th frame.\n",
      "frame_summaries.json 1731 to 577 frames.\n",
      "sequence_summary.json time_series -> 1731 to 577 entries\n",
      "File size before: 2828.24 KB, after: 944.26 KB\n",
      "Token count before: 716328, after: 239188\n",
      "FPS before: 10.0, after: 3.3333333333333335\n",
      "FPS reduction: 3.33 FPS\n",
      "Token reduction: 66.61%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 17\n",
      "17: 491 frames, keeping all (no condensation).\n",
      "frame_summaries.json 491 to 491 frames.\n",
      "sequence_summary.json time_series -> 491 to 491 entries\n",
      "File size before: 732.65 KB, after: 732.65 KB\n",
      "Token count before: 185230, after: 185230\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 18\n",
      "18: 1801 frames, keeping every 3th frame.\n",
      "frame_summaries.json 1801 to 601 frames.\n",
      "sequence_summary.json time_series -> 1801 to 601 entries\n",
      "File size before: 3069.74 KB, after: 1024.99 KB\n",
      "Token count before: 783617, after: 261649\n",
      "FPS before: 10.0, after: 3.3333333333333335\n",
      "FPS reduction: 3.33 FPS\n",
      "Token reduction: 66.61%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 19\n",
      "19: 4981 frames, keeping every 7th frame.\n",
      "frame_summaries.json 4981 to 712 frames.\n",
      "sequence_summary.json time_series -> 4981 to 712 entries\n",
      "File size before: 7951.24 KB, after: 1137.30 KB\n",
      "Token count before: 2013292, after: 287911\n",
      "FPS before: 10.0, after: 1.4285714285714286\n",
      "FPS reduction: 1.43 FPS\n",
      "Token reduction: 85.70%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 20\n",
      "20: 831 frames, keeping all (step=2 yields < 600 frames).\n",
      "frame_summaries.json 831 to 831 frames.\n",
      "sequence_summary.json time_series -> 831 to 831 entries\n",
      "File size before: 1368.63 KB, after: 1368.63 KB\n",
      "Token count before: 349533, after: 349533\n",
      "FPS before: 10.0, after: 10.0\n",
      "FPS reduction: 10.00 FPS\n",
      "Token reduction: 0.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Processing 21\n",
      "21: 2721 frames, keeping every 4th frame.\n",
      "frame_summaries.json 2721 to 681 frames.\n",
      "sequence_summary.json time_series -> 2721 to 681 entries\n",
      "File size before: 3876.95 KB, after: 971.15 KB\n",
      "Token count before: 975723, after: 244412\n",
      "FPS before: 10.0, after: 2.5\n",
      "FPS reduction: 2.50 FPS\n",
      "Token reduction: 74.95%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "\n",
    "# Define the range of folders (00 to 10) and exclude 08\n",
    "folders = [f\"{i:02d}\" for i in range(22)]\n",
    "\n",
    "# Base directory for input and output\n",
    "base_dir = Path.cwd()\n",
    "condensed_dir = base_dir / \"condensed\"\n",
    "\n",
    "# Ensure condensed directory exists\n",
    "condensed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Data storage for visualizations\n",
    "file_sizes_before = []\n",
    "file_sizes_after = []\n",
    "token_counts_before = []\n",
    "token_counts_after = []\n",
    "fps_before = []\n",
    "fps_after = []\n",
    "sequence_labels = []\n",
    "\n",
    "# Original FPS\n",
    "ORIGINAL_FPS = 10.0\n",
    "\n",
    "# Token counter using tiktoken\n",
    "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "for folder in folders:\n",
    "    # Input and output paths for frame_summaries.json\n",
    "    input_frame_path = base_dir / folder / \"frame_summaries.json\"\n",
    "    output_folder = condensed_dir / folder\n",
    "    output_frame_path = output_folder / \"frame_summaries.json\"\n",
    "    \n",
    "    # Input and output paths for sequence_summary.json\n",
    "    input_sequence_path = base_dir / folder / \"sequence_summary.json\"\n",
    "    output_sequence_path = output_folder / \"sequence_summary.json\"\n",
    "    \n",
    "    # Check if frame_summaries.json exists\n",
    "    if not input_frame_path.exists():\n",
    "        print(f\"Warning: {input_frame_path} does not exist, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Create output folder\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Calculate file size before for frame_summaries.json\n",
    "    frame_size_before = os.path.getsize(input_frame_path) / 1024  # Size in KB\n",
    "    sequence_size_before = 0\n",
    "    if input_sequence_path.exists():\n",
    "        sequence_size_before = os.path.getsize(input_sequence_path) / 1024\n",
    "    \n",
    "    # Read frame_summaries.json\n",
    "    with open(input_frame_path, 'r') as f:\n",
    "        frame_data = json.load(f)\n",
    "    \n",
    "    # Count tokens before for frame_summaries.json\n",
    "    frame_tokens_before = count_tokens(json.dumps(frame_data))\n",
    "    sequence_tokens_before = 0\n",
    "    if input_sequence_path.exists():\n",
    "        with open(input_sequence_path, 'r') as f:\n",
    "            sequence_data = json.load(f)\n",
    "        sequence_tokens_before = count_tokens(json.dumps(sequence_data))\n",
    "\n",
    "    print(f\"Processing {folder}\")\n",
    "    \n",
    "    # Determine condensation rule based on number of frames\n",
    "    num_frames = len(frame_data)\n",
    "    if num_frames <= 800:\n",
    "        step = 1\n",
    "        condensed_frame_data = frame_data\n",
    "        print(f\"{folder}: {num_frames} frames, keeping all (no condensation).\")\n",
    "    elif 801 <= num_frames <= 1600:\n",
    "        # Use step = 2 if it results in >= 600 frames, else step = 1\n",
    "        if math.ceil(num_frames / 2) >= 600:\n",
    "            step = 2\n",
    "            condensed_frame_data = frame_data[::2]\n",
    "            print(f\"{folder}: {num_frames} frames, keeping every 2nd frame.\")\n",
    "        else:\n",
    "            step = 1\n",
    "            condensed_frame_data = frame_data\n",
    "            print(f\"{folder}: {num_frames} frames, keeping all (step=2 yields < 600 frames).\")\n",
    "    else:  # num_frames > 1600\n",
    "        step = math.ceil(num_frames / 800)\n",
    "        condensed_frame_data = frame_data[::step]\n",
    "        print(f\"{folder}: {num_frames} frames, keeping every {step}th frame.\")\n",
    "    \n",
    "    # Write condensed frame_summaries.json\n",
    "    with open(output_frame_path, 'w') as f:\n",
    "        json.dump(condensed_frame_data, f, indent=4)\n",
    "    \n",
    "    # Calculate file size and tokens after for frame_summaries.json\n",
    "    frame_size_after = os.path.getsize(output_frame_path) / 1024\n",
    "    frame_tokens_after = count_tokens(json.dumps(condensed_frame_data))\n",
    "    \n",
    "    print(f\"frame_summaries.json {num_frames} to {len(condensed_frame_data)} frames.\")\n",
    "    \n",
    "    sequence_size_after = 0\n",
    "    sequence_tokens_after = 0\n",
    "    condensed_time_series = []\n",
    "    \n",
    "    # Process sequence_summary.json\n",
    "    if input_sequence_path.exists():\n",
    "        # Read sequence_summary.json\n",
    "        with open(input_sequence_path, 'r') as f:\n",
    "            sequence_data = json.load(f)\n",
    "        \n",
    "        # Apply the same condensation rule to time_series\n",
    "        if 'time_series' in sequence_data:\n",
    "            time_series = sequence_data['time_series']\n",
    "            condensed_time_series = time_series[::step]\n",
    "            sequence_data['time_series'] = condensed_time_series\n",
    "            \n",
    "            # Write updated sequence_summary.json\n",
    "            with open(output_sequence_path, 'w') as f:\n",
    "                json.dump(sequence_data, f, indent=4)\n",
    "            \n",
    "            # Calculate file size and tokens after for sequence_summary.json\n",
    "            sequence_size_after = os.path.getsize(output_sequence_path) / 1024\n",
    "            sequence_tokens_after = count_tokens(json.dumps(sequence_data))\n",
    "            \n",
    "            print(f\"sequence_summary.json time_series -> {len(time_series)} to {len(condensed_time_series)} entries\")\n",
    "        else:\n",
    "            print(f\"Warning: 'time_series' not found in {input_sequence_path}, skipping condensation for sequence_summary.json.\")\n",
    "    else:\n",
    "        print(f\"Warning: {input_sequence_path} does not exist, skipping.\")\n",
    "\n",
    "    print(f\"File size before: {frame_size_before + sequence_size_before:.2f} KB, after: {frame_size_after + sequence_size_after:.2f} KB\")\n",
    "    print(f\"Token count before: {frame_tokens_before + sequence_tokens_before}, after: {frame_tokens_after + sequence_tokens_after}\")\n",
    "    print(f\"FPS before: {ORIGINAL_FPS}, after: {ORIGINAL_FPS / step}\")\n",
    "    print(f\"FPS reduction: {ORIGINAL_FPS / step:.2f} FPS\")\n",
    "    print(f\"Token reduction: {((frame_tokens_before + sequence_tokens_before) - (frame_tokens_after + sequence_tokens_after)) / (frame_tokens_before + sequence_tokens_before) * 100:.2f}%\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Store data for visualizations\n",
    "    sequence_labels.append(folder)\n",
    "    file_sizes_before.append(frame_size_before + sequence_size_before)\n",
    "    file_sizes_after.append(frame_size_after + sequence_size_after)\n",
    "    token_counts_before.append(frame_tokens_before + sequence_tokens_before)\n",
    "    token_counts_after.append(frame_tokens_after + sequence_tokens_after)\n",
    "    fps_before.append(ORIGINAL_FPS)\n",
    "    fps_after.append(ORIGINAL_FPS / step)\n",
    "\n",
    "# Visualization 1: File Size Reduction\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(sequence_labels))\n",
    "plt.bar([i - 0.2 for i in x], file_sizes_before, width=0.4, label='Before', color='skyblue')\n",
    "plt.bar([i + 0.2 for i in x], file_sizes_after, width=0.4, label='After', color='salmon')\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Total File Size (KB)')\n",
    "plt.title('File Size Reduction Before and After Condensation')\n",
    "plt.xticks(x, sequence_labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_dir / 'file_size_reduction.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization 2: Percentage Token Reduction\n",
    "token_reduction_percent = [((before - after) / before * 100) if before > 0 else 0 \n",
    "                           for before, after in zip(token_counts_before, token_counts_after)]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sequence_labels, token_reduction_percent, color='lightgreen')\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Token Reduction (%)')\n",
    "plt.title('Percentage Token Reduction After Condensation')\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_dir / 'token_reduction.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization 3: FPS Reduction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([i - 0.2 for i in x], fps_before, width=0.4, label='Before', color='lightblue')\n",
    "plt.bar([i + 0.2 for i in x], fps_after, width=0.4, label='After', color='coral')\n",
    "plt.xlabel('Sequence')\n",
    "plt.ylabel('Frames Per Second (FPS)')\n",
    "plt.title('FPS Before and After Condensation')\n",
    "plt.xticks(x, sequence_labels)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_dir / 'fps_reduction.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb369af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
